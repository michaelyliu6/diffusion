{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Guidance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "# Define the model\n",
    "class FashionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionCNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(in_features=64 * 6 * 6, out_features=600)\n",
    "        self.drop = nn.Dropout2d(0.25)\n",
    "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
    "        self.fc3 = nn.Linear(in_features=120, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Initialize wandb\n",
    "# wandb.init(project=\"fashion-mnist-classifier\")\n",
    "\n",
    "# # Hyperparameters\n",
    "# config = wandb.config\n",
    "# config.batch_size = 100\n",
    "# config.learning_rate = 0.001\n",
    "# config.num_epochs = 25\n",
    "# config.test_every_n_epochs = 3  # Test every 3 epochs\n",
    "\n",
    "# # Device configuration\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else 'cpu')\n",
    "\n",
    "# # Load FashionMNIST dataset\n",
    "# train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5,), (0.5,))\n",
    "# ]), download=True)\n",
    "\n",
    "# test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5,), (0.5,))\n",
    "# ]))\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(dataset=test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "# # Initialize model, loss function, and optimizer\n",
    "# model = FashionCNN().to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# # Watch the model\n",
    "# wandb.watch(model)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(config.num_epochs):\n",
    "#     model.train()\n",
    "#     epoch_loss = 0\n",
    "#     epoch_accuracy = 0\n",
    "#     for i, (images, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.num_epochs} (Train)\")):\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward and optimize\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Calculate accuracy\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         batch_accuracy = (predicted == labels).sum().item() / labels.size(0)\n",
    "\n",
    "#         epoch_loss += loss.item()\n",
    "#         epoch_accuracy += batch_accuracy\n",
    "#         wandb.log({\"batch_loss\": loss.item(), \"batch_accuracy\": batch_accuracy})\n",
    "\n",
    "#     avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "#     avg_epoch_accuracy = epoch_accuracy / len(train_loader)\n",
    "#     wandb.log({\"epoch\": epoch + 1, \"epoch_loss\": avg_epoch_loss, \"epoch_accuracy\": avg_epoch_accuracy})\n",
    "#     print(f'Epoch [{epoch+1}/{config.num_epochs}], Train Loss: {avg_epoch_loss:.4f}, Train Accuracy: {avg_epoch_accuracy:.4f}')\n",
    "\n",
    "#     # Test during training\n",
    "#     if (epoch + 1) % config.test_every_n_epochs == 0:\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             correct = 0\n",
    "#             total = 0\n",
    "#             test_loss = 0\n",
    "#             for images, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{config.num_epochs} (Test)\"):\n",
    "#                 images = images.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "#                 outputs = model(images)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "#                 test_loss += loss.item()\n",
    "\n",
    "#                 _, predicted = torch.max(outputs.data, 1)\n",
    "#                 total += labels.size(0)\n",
    "#                 correct += (predicted == labels).sum().item()\n",
    "\n",
    "#             avg_test_loss = test_loss / len(test_loader)\n",
    "#             test_accuracy = 100 * correct / total\n",
    "#             wandb.log({\"epoch\": epoch + 1, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy})\n",
    "#             print(f'Epoch [{epoch+1}/{config.num_epochs}], Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f} %')\n",
    "\n",
    "# wandb.finish()\n",
    "\n",
    "# # Save the model checkpoint\n",
    "# torch.save(model.state_dict(), 'fashion_mnist_cnn.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"references/classifier_guided_DDPM.png\" alt=\"Positional Embedding\">\n",
    "<figcaption>From Calvo (2018) in <a href=\"https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3\">Dissecting BERT Part 1: The Encoder</a> (different blog post from the one previously mentioned)</figcaption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sm/byz3skxx5c57sw2mzmy66j240000gn/T/ipykernel_19248/3120068008.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  diffusion_model.load_state_dict(t.load(str(MODEL_FILENAME)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel sizes for in/out: [(28, 28), (28, 56), (56, 112)]\n",
      "Channel sizes for in/out: [(28, 28), (28, 56), (56, 112)]\n"
     ]
    }
   ],
   "source": [
    "diffusion_model = Unet(\n",
    "    max_steps=config_dict[\"max_steps\"],\n",
    "    channels=config_dict[\"model_channels\"],\n",
    "    image_shape=config_dict[\"image_shape\"],\n",
    "    dim_mults=config_dict[\"model_dim_mults\"],\n",
    ").to(config_dict[\"device\"])\n",
    "assert isinstance(data_folder, Path)\n",
    "MODEL_FILENAME = data_folder / \"unet_model_thousand.pt\"\n",
    "if MODEL_FILENAME.exists():\n",
    "    diffusion_model = Unet(\n",
    "        max_steps=config_dict[\"max_steps\"],\n",
    "        channels=config_dict[\"model_channels\"],\n",
    "        image_shape=config_dict[\"image_shape\"],\n",
    "        dim_mults=config_dict[\"model_dim_mults\"],\n",
    "    ).to(config_dict[\"device\"])\n",
    "    diffusion_model.noise_schedule = NoiseSchedule(config_dict[\"max_steps\"], config_dict[\"device\"])\n",
    "    diffusion_model.load_state_dict(t.load(str(MODEL_FILENAME)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/michaelyliu6/fashionmnist-classifier/commit/a048c8a3263fbbe64768787a8f7eb1f634b121f2', commit_message='Upload fashionmnist-classifier-simple.pt with huggingface_hub', commit_description='', oid='a048c8a3263fbbe64768787a8f7eb1f634b121f2', pr_url=None, repo_url=RepoUrl('https://huggingface.co/michaelyliu6/fashionmnist-classifier', endpoint='https://huggingface.co', repo_type='model', repo_id='michaelyliu6/fashionmnist-classifier'), pr_revision=None, pr_num=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "api.upload_file(\n",
    "    path_or_fileobj=\"./data/w3d4/simple_classifier.pt\",\n",
    "    path_in_repo=\"fashionmnist-classifier-simple.pt\",\n",
    "    repo_id=\"michaelyliu6/fashionmnist-classifier\",\n",
    "    repo_type=\"model\",\n",
    "    create_pr=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sm/byz3skxx5c57sw2mzmy66j240000gn/T/ipykernel_19248/1467744199.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('fashion_mnist_cnn.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = FashionCNN().to('mps')\n",
    "model.load_state_dict(torch.load('fashion_mnist_cnn.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sample_images_from_class(\n",
    "    diffusion_model,\n",
    "    classifier_model,\n",
    "    class_index,\n",
    "    num_images,\n",
    "    num_sampling_steps,\n",
    "    image_size,\n",
    "    guidance_scale=1.5,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates images of a specific class using classifier-guided diffusion.\n",
    "\n",
    "    Args:\n",
    "        diffusion_model: Your trained DDPM model (with Unet and NoiseSchedule).\n",
    "        classifier_model: The trained classifier model.\n",
    "        class_index: The index of the class to generate (0 to num_classes-1).\n",
    "        num_images: The number of images to generate.\n",
    "        num_sampling_steps: The number of diffusion timesteps for sampling.\n",
    "        image_size: The size of the generated images (e.g., (1, 28, 28) for FashionMNIST).\n",
    "        guidance_scale: The scale factor for classifier guidance.\n",
    "        device: The device to perform computations on.\n",
    "\n",
    "    Returns:\n",
    "        A tensor of generated images.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Get Noise Schedule:\n",
    "    noise_schedule = diffusion_model.noise_schedule\n",
    "\n",
    "    # 2. Initialize Samples:\n",
    "    x = torch.randn((num_images, *image_size), device=device)\n",
    "\n",
    "    # 3. Class Labels (for conditional sampling):\n",
    "    # Create labels all corresponding to the desired class_index\n",
    "    labels = torch.full((num_images,), class_index, device=device, dtype=torch.long)\n",
    "    one_hot_labels = F.one_hot(\n",
    "        labels, num_classes=classifier_model.fc2.out_features\n",
    "    ).float()  # Get num_classes from classifier\n",
    "\n",
    "    # 4. Sampling Loop:\n",
    "    for i in tqdm(\n",
    "        reversed(range(0, num_sampling_steps)),\n",
    "        desc=\"Sampling\",\n",
    "        total=num_sampling_steps,\n",
    "    ):\n",
    "        t = torch.full((num_images,), i, device=device, dtype=torch.long)\n",
    "\n",
    "        # 4.1. Classifier Prediction (and Gradient Calculation):\n",
    "        with torch.enable_grad():\n",
    "            x_in = x.detach().requires_grad_(True)\n",
    "            classifier_logits = classifier_model(x_in)\n",
    "            log_probs = F.log_softmax(classifier_logits, dim=-1)\n",
    "            selected = log_probs[range(len(log_probs)), labels]  # Select log_probs of the desired class\n",
    "            classifier_grad = (\n",
    "                torch.autograd.grad(selected.sum(), x_in)[0] * guidance_scale\n",
    "            )\n",
    "\n",
    "        # 4.2. DDPM Denoising Step:\n",
    "        model_output = diffusion_model(x, t)\n",
    "\n",
    "        # 4.3. Apply Classifier Guidance:\n",
    "        alpha_t = noise_schedule.alpha(t)\n",
    "        alpha_bar_t = noise_schedule.alpha_bar(t)\n",
    "        beta_t = noise_schedule.beta(t)\n",
    "        sigma_t = torch.sqrt(beta_t)\n",
    "\n",
    "        # Apply guidance to modify the model output (predicted noise)\n",
    "        if guidance_scale > 0.0:\n",
    "            model_output = model_output - torch.sqrt(1 - alpha_bar_t).view(-1, 1, 1, 1) * classifier_grad\n",
    "\n",
    "        # 4.4. Predict Previous Sample (x_{t-1}): using the modified model_output\n",
    "        pred_mean = (\n",
    "            1\n",
    "            / torch.sqrt(alpha_t)\n",
    "        ).view(-1, 1, 1, 1) * (\n",
    "            x\n",
    "            - (1 - alpha_t).view(-1, 1, 1, 1)\n",
    "            / torch.sqrt(1 - alpha_bar_t).view(-1, 1, 1, 1)\n",
    "            * model_output\n",
    "        )\n",
    "        \n",
    "        if i > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "        else:\n",
    "            noise = torch.zeros_like(x)\n",
    "\n",
    "        x = pred_mean + sigma_t.view(-1, 1, 1, 1) * noise\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 1000/1000 [01:23<00:00, 11.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Example: Generate 16 images of class 3 (e.g., \"Dress\" in FashionMNIST)\n",
    "\n",
    "# 0 T-shirt/top\n",
    "# 1 Trouser\n",
    "# 2 Pullover\n",
    "# 3 Dress\n",
    "# 4 Coat\n",
    "# 5 Sandal\n",
    "# 6 Shirt\n",
    "# 7 Sneaker\n",
    "# 8 Bag\n",
    "# 9 Ankle boot\n",
    "\n",
    "class_index = 9  # The class you want to generate\n",
    "num_images = 1\n",
    "image_size = (1, 28, 28)\n",
    "guidance_scale = 3\n",
    "model = model.to(\"mps\")\n",
    "\n",
    "generated_images = sample_images_from_class(\n",
    "    diffusion_model,\n",
    "    model,\n",
    "    class_index,\n",
    "    num_images,\n",
    "    num_sampling_steps=diffusion_model.noise_schedule.max_steps,\n",
    "    image_size=image_size,\n",
    "    guidance_scale=guidance_scale,\n",
    "    device=\"mps\",  # Or \"cpu\"\n",
    ")\n",
    "\n",
    "# generated_images now contains the generated images (no labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=112x112>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_image_grid(generated_images.detach().cpu(), images_per_row=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
